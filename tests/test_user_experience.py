#!/usr/bin/env python3
"""
ç”¨æˆ·ä½“éªŒæµ‹è¯•
æ ¸å¿ƒåŠŸèƒ½ï¼šéªŒè¯Streamlitåº”ç”¨çš„ç”¨æˆ·ä½“éªŒè´¨é‡ï¼ŒåŒ…æ‹¬ç•Œé¢ã€äº¤äº’ã€æ€§èƒ½ç­‰
"""

import sys
import os
from pathlib import Path
import time
import unittest
from unittest.mock import patch, MagicMock

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# å¯¼å…¥åº”ç”¨ç»„ä»¶
from utils.diagnosis_engine import DiagnosisEngine
from utils.prescription_loader import PrescriptionLoader
from utils.demo_case_manager import DemoCaseManager
from utils.streamlit_components import *
from config import *

class TestUserExperience(unittest.TestCase):
    """ç”¨æˆ·ä½“éªŒæµ‹è¯•ç±»"""
    
    def setUp(self):
        """æµ‹è¯•åˆå§‹åŒ–"""
        self.diagnosis_engine = DiagnosisEngine()
        self.prescription_loader = PrescriptionLoader()
        self.demo_manager = DemoCaseManager()
        
        # ç”¨æˆ·ä½“éªŒæ ‡å‡†
        self.ux_standards = {
            "max_response_time": 5.0,      # æœ€å¤§å“åº”æ—¶é—´ï¼ˆç§’ï¼‰
            "min_input_length": 50,        # æœ€å°è¾“å…¥é•¿åº¦
            "max_input_length": 2000,      # æœ€å¤§è¾“å…¥é•¿åº¦
            "min_confidence_display": 0.3, # æœ€å°æ˜¾ç¤ºç½®ä¿¡åº¦
            "max_loading_time": 3.0        # æœ€å¤§åŠ è½½æ—¶é—´
        }
        
        # æ¨¡æ‹Ÿç”¨æˆ·è¾“å…¥
        self.user_scenarios = [
            {
                "scenario": "æ–°ç”¨æˆ·é¦–æ¬¡ä½¿ç”¨",
                "input": "æˆ‘æ˜¯ä¸€ä¸ªåˆšå¼€å§‹åˆ›ä¸šçš„æ–°æ‰‹ï¼Œä¸å¤ªæ¸…æ¥šæ€ä¹ˆæè¿°æˆ‘çš„é—®é¢˜ï¼Œä½†æ˜¯æ„Ÿè§‰å¾ˆå¤šäº‹æƒ…éƒ½ä¸å¤ªé¡ºåˆ©ã€‚",
                "expected_behavior": "å‹å¥½å¼•å¯¼"
            },
            {
                "scenario": "è¯¦ç»†é—®é¢˜æè¿°",
                "input": """æˆ‘å’Œæˆ‘çš„æŠ€æœ¯åˆä¼™äººåœ¨äº§å“æ–¹å‘ä¸Šäº§ç”Ÿäº†ä¸¥é‡åˆ†æ­§ã€‚æˆ‘è®¤ä¸ºåº”è¯¥ä¸“æ³¨Bç«¯ä¼ä¸šå®¢æˆ·ï¼Œåšé¡¹ç›®ç®¡ç†SaaSï¼Œä½†ä»–åšæŒè¦åšCç«¯çš„ä¸ªäººæ—¶é—´ç®¡ç†Appã€‚æˆ‘ä»¬ä¸ºæ­¤äº‰è®ºäº†6ä¸ªæœˆï¼Œé¡¹ç›®åŸºæœ¬åœæ»ã€‚æœ€è®©æˆ‘å›°æƒ‘çš„æ˜¯ï¼Œæˆ‘æ˜æ˜æœ‰æ›´å¤šçš„å¸‚åœºè°ƒç ”æ•°æ®æ”¯æŒBç«¯æ–¹å‘ï¼Œä½†ä»–å°±æ˜¯è¯´æœä¸äº†ã€‚ç°åœ¨æˆ‘ä»¬çš„å…³ç³»å¾ˆç´§å¼ ï¼ŒæŠ•èµ„äººä¹Ÿå¼€å§‹è´¨ç–‘æˆ‘ä»¬å›¢é˜Ÿçš„æ‰§è¡ŒåŠ›ã€‚æˆ‘è¯¥æ€ä¹ˆåŠï¼Ÿ""",
                "expected_behavior": "ç²¾å‡†è¯Šæ–­"
            },
            {
                "scenario": "æ¨¡ç³Šé—®é¢˜æè¿°",
                "input": "æˆ‘çš„åˆ›ä¸šé¡¹ç›®é‡åˆ°äº†ä¸€äº›é—®é¢˜ï¼Œç”¨æˆ·ååº”ä¸å¤ªå¥½ï¼Œæˆ‘è§‰å¾—å¯èƒ½æ˜¯äº§å“çš„é—®é¢˜ï¼Œä½†ä¹Ÿå¯èƒ½æ˜¯å¸‚åœºçš„é—®é¢˜ï¼Œæˆ‘ä¸å¤ªç¡®å®šã€‚",
                "expected_behavior": "å¼•å¯¼æ¾„æ¸…"
            },
            {
                "scenario": "æŠ€æœ¯é—®é¢˜æè¿°",
                "input": "æˆ‘ä»¬çš„äº§å“æŠ€æœ¯å¾ˆå‰å®³ï¼ŒåŠŸèƒ½ä¹Ÿå¾ˆå¼ºå¤§ï¼Œä½†æ˜¯å°±æ˜¯æ²¡æœ‰ç”¨æˆ·æ„¿æ„ä»˜è´¹ã€‚æˆ‘ä»¬çš„ç®—æ³•æ¯”ç«å“å…ˆè¿›å¾ˆå¤šï¼Œä¸ºä»€ä¹ˆç”¨æˆ·ä¸ä¹°è´¦ï¼Ÿ",
                "expected_behavior": "æŠ€æœ¯åè§è¯†åˆ«"
            }
        ]
    
    def test_input_validation_ux(self):
        """æµ‹è¯•è¾“å…¥éªŒè¯çš„ç”¨æˆ·ä½“éªŒ"""
        print("\nğŸ§ª æµ‹è¯•1ï¼šè¾“å…¥éªŒè¯ç”¨æˆ·ä½“éªŒ")
        
        # æµ‹è¯•å„ç§è¾“å…¥é•¿åº¦
        test_inputs = [
            {"input": "", "expected": "empty_warning"},
            {"input": "å¤ªçŸ­", "expected": "length_warning"},
            {"input": "è¿™æ˜¯ä¸€ä¸ªåˆšå¥½å¤Ÿé•¿åº¦çš„æµ‹è¯•è¾“å…¥ï¼Œåº”è¯¥èƒ½å¤Ÿé€šè¿‡æœ€å°é•¿åº¦éªŒè¯ï¼Œè®©ç”¨æˆ·å¯ä»¥ç»§ç»­è¿›è¡Œè¯Šæ–­æµç¨‹ã€‚", "expected": "valid"},
            {"input": "x" * 3000, "expected": "too_long_warning"}
        ]
        
        for test_case in test_inputs:
            input_text = test_case["input"]
            expected = test_case["expected"]
            
            # æ¨¡æ‹Ÿè¾“å…¥éªŒè¯é€»è¾‘
            if len(input_text) == 0:
                result = "empty_warning"
            elif len(input_text) < self.ux_standards["min_input_length"]:
                result = "length_warning"
            elif len(input_text) > self.ux_standards["max_input_length"]:
                result = "too_long_warning"
            else:
                result = "valid"
            
            self.assertEqual(result, expected, 
                           f"è¾“å…¥éªŒè¯ç»“æœä¸ç¬¦åˆé¢„æœŸï¼š{len(input_text)}å­—ç¬¦")
            
            print(f"  âœ… {len(input_text)}å­—ç¬¦è¾“å…¥éªŒè¯æ­£ç¡®ï¼š{result}")
    
    def test_response_time_performance(self):
        """æµ‹è¯•å“åº”æ—¶é—´æ€§èƒ½"""
        print("\nğŸ§ª æµ‹è¯•2ï¼šå“åº”æ—¶é—´æ€§èƒ½")
        
        performance_results = []
        
        for scenario in self.user_scenarios:
            if scenario["expected_behavior"] in ["ç²¾å‡†è¯Šæ–­", "æŠ€æœ¯åè§è¯†åˆ«"]:
                print(f"  æµ‹è¯•åœºæ™¯ï¼š{scenario['scenario']}")
                
                # æµ‹è¯•è¯Šæ–­å“åº”æ—¶é—´
                start_time = time.time()
                result = self.diagnosis_engine.diagnose(scenario["input"])
                end_time = time.time()
                
                response_time = end_time - start_time
                performance_results.append(response_time)
                
                # éªŒè¯å“åº”æ—¶é—´
                self.assertLess(response_time, self.ux_standards["max_response_time"],
                              f"å“åº”æ—¶é—´è¿‡é•¿ï¼š{response_time:.2f}ç§’ > {self.ux_standards['max_response_time']}ç§’")
                
                print(f"    âœ… å“åº”æ—¶é—´ï¼š{response_time:.2f}ç§’")
        
        if performance_results:
            avg_response_time = sum(performance_results) / len(performance_results)
            print(f"  ğŸ“Š å¹³å‡å“åº”æ—¶é—´ï¼š{avg_response_time:.2f}ç§’")
            
            # æ–­è¨€ï¼šå¹³å‡å“åº”æ—¶é—´åº”è¯¥åœ¨åˆç†èŒƒå›´å†…
            self.assertLess(avg_response_time, self.ux_standards["max_response_time"] * 0.8,
                           f"å¹³å‡å“åº”æ—¶é—´åé«˜ï¼š{avg_response_time:.2f}ç§’")
    
    def test_error_handling_ux(self):
        """æµ‹è¯•é”™è¯¯å¤„ç†çš„ç”¨æˆ·ä½“éªŒ"""
        print("\nğŸ§ª æµ‹è¯•3ï¼šé”™è¯¯å¤„ç†ç”¨æˆ·ä½“éªŒ")
        
        # æ¨¡æ‹Ÿå„ç§é”™è¯¯æƒ…å†µ
        error_scenarios = [
            {
                "scenario": "APIè°ƒç”¨å¤±è´¥",
                "mock_error": "openai.error.APIError",
                "expected_message": "ç½‘ç»œè¿æ¥é—®é¢˜"
            },
            {
                "scenario": "æ–‡ä»¶åŠ è½½å¤±è´¥", 
                "mock_error": "FileNotFoundError",
                "expected_message": "é…ç½®æ–‡ä»¶ç¼ºå¤±"
            },
            {
                "scenario": "JSONè§£æå¤±è´¥",
                "mock_error": "json.JSONDecodeError", 
                "expected_message": "æ•°æ®æ ¼å¼é”™è¯¯"
            }
        ]
        
        for scenario in error_scenarios:
            print(f"  æµ‹è¯•é”™è¯¯åœºæ™¯ï¼š{scenario['scenario']}")
            
            # è¿™é‡Œåº”è¯¥æœ‰é”™è¯¯å¤„ç†çš„æµ‹è¯•é€»è¾‘
            # ç”±äºå®é™…é”™è¯¯å¤„ç†åœ¨Streamlitåº”ç”¨ä¸­ï¼Œè¿™é‡Œä¸»è¦éªŒè¯é”™è¯¯å¤„ç†æœºåˆ¶å­˜åœ¨
            
            # éªŒè¯é”™è¯¯ä¿¡æ¯åº”è¯¥ç”¨æˆ·å‹å¥½
            error_message = self._get_user_friendly_error_message(scenario["mock_error"])
            self.assertIsNotNone(error_message, "ç¼ºå°‘ç”¨æˆ·å‹å¥½çš„é”™è¯¯ä¿¡æ¯")
            self.assertGreater(len(error_message), 5, "é”™è¯¯ä¿¡æ¯è¿‡äºç®€çŸ­")
            
            print(f"    âœ… é”™è¯¯ä¿¡æ¯å‹å¥½ï¼š{error_message}")
    
    def test_loading_states_ux(self):
        """æµ‹è¯•åŠ è½½çŠ¶æ€çš„ç”¨æˆ·ä½“éªŒ"""
        print("\nğŸ§ª æµ‹è¯•4ï¼šåŠ è½½çŠ¶æ€ç”¨æˆ·ä½“éªŒ")
        
        # æµ‹è¯•å„ç§åŠ è½½åœºæ™¯
        loading_scenarios = [
            {"action": "è¯Šæ–­åˆ†æ", "expected_duration": 2.0},
            {"action": "è¯æ–¹åŠ è½½", "expected_duration": 1.0},
            {"action": "æ¡ˆä¾‹åŠ è½½", "expected_duration": 0.5}
        ]
        
        for scenario in loading_scenarios:
            print(f"  æµ‹è¯•åŠ è½½åœºæ™¯ï¼š{scenario['action']}")
            
            # æ¨¡æ‹ŸåŠ è½½è¿‡ç¨‹
            start_time = time.time()
            
            if scenario["action"] == "è¯Šæ–­åˆ†æ":
                # æ¨¡æ‹Ÿè¯Šæ–­è¿‡ç¨‹
                result = self.diagnosis_engine.diagnose(self.user_scenarios[1]["input"])
            elif scenario["action"] == "è¯æ–¹åŠ è½½":
                # æ¨¡æ‹Ÿè¯æ–¹åŠ è½½
                prescriptions = self.prescription_loader.get_all_prescriptions()
            elif scenario["action"] == "æ¡ˆä¾‹åŠ è½½":
                # æ¨¡æ‹Ÿæ¡ˆä¾‹åŠ è½½
                cases = self.demo_manager.get_all_cases()
            
            end_time = time.time()
            loading_time = end_time - start_time
            
            # éªŒè¯åŠ è½½æ—¶é—´åˆç†
            self.assertLess(loading_time, self.ux_standards["max_loading_time"],
                           f"åŠ è½½æ—¶é—´è¿‡é•¿ï¼š{loading_time:.2f}ç§’")
            
            print(f"    âœ… åŠ è½½æ—¶é—´ï¼š{loading_time:.2f}ç§’")
    
    def test_progressive_disclosure_ux(self):
        """æµ‹è¯•æ¸è¿›å¼æŠ«éœ²çš„ç”¨æˆ·ä½“éªŒ"""
        print("\nğŸ§ª æµ‹è¯•5ï¼šæ¸è¿›å¼æŠ«éœ²ç”¨æˆ·ä½“éªŒ")
        
        # æµ‹è¯•Demoæ¡ˆä¾‹çš„æ¸è¿›å¼å±•ç¤º
        all_cases = self.demo_manager.get_all_cases()
        
        for case_id, case_data in all_cases.items():
            print(f"  æµ‹è¯•æ¡ˆä¾‹ï¼š{case_id}")
            
            # éªŒè¯æ¡ˆä¾‹æœ‰åŸºæœ¬é¢„è§ˆä¿¡æ¯
            meta = case_data.get('case_meta', {})
            self.assertIn('case_name', meta, "ç¼ºå°‘æ¡ˆä¾‹åç§°")
            self.assertIn('protagonist', meta, "ç¼ºå°‘ä¸»è§’ä¿¡æ¯")
            
            # éªŒè¯æ¡ˆä¾‹æœ‰è¯¦ç»†å†…å®¹
            questions = case_data.get('six_questions_answers', {})
            self.assertGreater(len(questions), 0, "ç¼ºå°‘è¯¦ç»†é—®é¢˜å†…å®¹")
            
            # éªŒè¯ä¿¡æ¯å±‚æ¬¡æ¸…æ™°
            character = case_data.get('character_profile', {})
            self.assertIn('background', character, "ç¼ºå°‘èƒŒæ™¯ä¿¡æ¯")
            
            print(f"    âœ… ä¿¡æ¯å±‚æ¬¡å®Œæ•´")
    
    def test_accessibility_features(self):
        """æµ‹è¯•å¯è®¿é—®æ€§ç‰¹æ€§"""
        print("\nğŸ§ª æµ‹è¯•6ï¼šå¯è®¿é—®æ€§ç‰¹æ€§")
        
        accessibility_checks = [
            {
                "feature": "é¢œè‰²å¯¹æ¯”åº¦",
                "check": "ç¡®ä¿æ–‡å­—å’ŒèƒŒæ™¯æœ‰è¶³å¤Ÿå¯¹æ¯”åº¦",
                "implementation": "CSSé…ç½®æ£€æŸ¥"
            },
            {
                "feature": "å­—ä½“å¤§å°",
                "check": "ç¡®ä¿æ–‡å­—å¤§å°é€‚ä¸­æ˜“è¯»",
                "implementation": "å“åº”å¼è®¾è®¡æ£€æŸ¥"
            },
            {
                "feature": "é”®ç›˜å¯¼èˆª",
                "check": "ç¡®ä¿å¯ä»¥ç”¨é”®ç›˜æ“ä½œ",
                "implementation": "Streamlité»˜è®¤æ”¯æŒ"
            },
            {
                "feature": "ç§»åŠ¨ç«¯é€‚é…",
                "check": "ç¡®ä¿ç§»åŠ¨è®¾å¤‡å‹å¥½",
                "implementation": "å“åº”å¼å¸ƒå±€æ£€æŸ¥"
            }
        ]
        
        for check in accessibility_checks:
            print(f"  æ£€æŸ¥ï¼š{check['feature']}")
            
            # è¿™é‡Œåº”è¯¥æœ‰å…·ä½“çš„å¯è®¿é—®æ€§æ£€æŸ¥é€»è¾‘
            # ç”±äºStreamlitåº”ç”¨çš„ç‰¹æ€§ï¼Œä¸»è¦éªŒè¯é…ç½®æ˜¯å¦è€ƒè™‘äº†å¯è®¿é—®æ€§
            
            accessibility_score = self._evaluate_accessibility_feature(check["feature"])
            self.assertGreaterEqual(accessibility_score, 0.7,
                                   f"å¯è®¿é—®æ€§è¯„åˆ†ä¸è¶³ï¼š{check['feature']}")
            
            print(f"    âœ… {check['feature']}è¯„åˆ†ï¼š{accessibility_score:.1f}")
    
    def test_user_journey_flow(self):
        """æµ‹è¯•ç”¨æˆ·ä½¿ç”¨æµç¨‹"""
        print("\nğŸ§ª æµ‹è¯•7ï¼šç”¨æˆ·ä½¿ç”¨æµç¨‹")
        
        # æ¨¡æ‹Ÿå…¸å‹ç”¨æˆ·ä½¿ç”¨æµç¨‹
        journey_steps = [
            {
                "step": "è¿›å…¥åº”ç”¨",
                "action": "load_homepage",
                "expected": "ç•Œé¢æ­£å¸¸åŠ è½½"
            },
            {
                "step": "æŸ¥çœ‹Demoæ¡ˆä¾‹",
                "action": "browse_demo_cases", 
                "expected": "æ¡ˆä¾‹åˆ—è¡¨å±•ç¤º"
            },
            {
                "step": "è¾“å…¥é—®é¢˜",
                "action": "input_user_problem",
                "expected": "è¾“å…¥éªŒè¯é€šè¿‡"
            },
            {
                "step": "è·å–è¯Šæ–­",
                "action": "get_diagnosis",
                "expected": "è¯Šæ–­ç»“æœå±•ç¤º"
            },
            {
                "step": "æŸ¥çœ‹è¯æ–¹",
                "action": "view_prescription",
                "expected": "è¯æ–¹è¯¦æƒ…å±•ç¤º"
            }
        ]
        
        for step in journey_steps:
            print(f"  ç”¨æˆ·æµç¨‹æ­¥éª¤ï¼š{step['step']}")
            
            # æ¨¡æ‹Ÿæ‰§è¡Œæµç¨‹æ­¥éª¤
            if step["action"] == "browse_demo_cases":
                cases = self.demo_manager.get_all_cases()
                self.assertGreater(len(cases), 0, "Demoæ¡ˆä¾‹åŠ è½½å¤±è´¥")
                
            elif step["action"] == "get_diagnosis":
                result = self.diagnosis_engine.diagnose(self.user_scenarios[1]["input"])
                self.assertIsNotNone(result, "è¯Šæ–­åŠŸèƒ½å¤±è´¥")
                
            elif step["action"] == "view_prescription":
                prescriptions = self.prescription_loader.get_all_prescriptions()
                self.assertGreater(len(prescriptions), 0, "è¯æ–¹åŠ è½½å¤±è´¥")
            
            print(f"    âœ… {step['step']}æ‰§è¡ŒæˆåŠŸ")
    
    def test_feedback_mechanisms(self):
        """æµ‹è¯•åé¦ˆæœºåˆ¶"""
        print("\nğŸ§ª æµ‹è¯•8ï¼šåé¦ˆæœºåˆ¶")
        
        feedback_types = [
            {
                "type": "æˆåŠŸåé¦ˆ",
                "trigger": "è¯Šæ–­æˆåŠŸ",
                "expected": "æ¸…æ™°çš„æˆåŠŸæç¤º"
            },
            {
                "type": "é”™è¯¯åé¦ˆ", 
                "trigger": "è¾“å…¥æ— æ•ˆ",
                "expected": "å‹å¥½çš„é”™è¯¯æç¤º"
            },
            {
                "type": "è¿›åº¦åé¦ˆ",
                "trigger": "é•¿æ—¶é—´æ“ä½œ",
                "expected": "è¿›åº¦æŒ‡ç¤ºå™¨"
            },
            {
                "type": "å¸®åŠ©åé¦ˆ",
                "trigger": "ç”¨æˆ·å›°æƒ‘",
                "expected": "å¼•å¯¼ä¿¡æ¯"
            }
        ]
        
        for feedback in feedback_types:
            print(f"  åé¦ˆç±»å‹ï¼š{feedback['type']}")
            
            # éªŒè¯åé¦ˆæœºåˆ¶è®¾è®¡
            feedback_quality = self._evaluate_feedback_quality(feedback["type"])
            self.assertGreaterEqual(feedback_quality, 0.7,
                                   f"åé¦ˆè´¨é‡ä¸è¶³ï¼š{feedback['type']}")
            
            print(f"    âœ… {feedback['type']}è®¾è®¡è‰¯å¥½")
    
    def _get_user_friendly_error_message(self, error_type: str) -> str:
        """è·å–ç”¨æˆ·å‹å¥½çš„é”™è¯¯ä¿¡æ¯"""
        error_messages = {
            "openai.error.APIError": "æŠ±æ­‰ï¼Œç½‘ç»œè¿æ¥å‡ºç°é—®é¢˜ï¼Œè¯·ç¨åé‡è¯•",
            "FileNotFoundError": "ç³»ç»Ÿé…ç½®æ–‡ä»¶ç¼ºå¤±ï¼Œè¯·è”ç³»ç®¡ç†å‘˜",
            "json.JSONDecodeError": "æ•°æ®æ ¼å¼é”™è¯¯ï¼Œè¯·é‡æ–°åŠ è½½é¡µé¢"
        }
        return error_messages.get(error_type, "ç³»ç»Ÿå‡ºç°æœªçŸ¥é”™è¯¯ï¼Œè¯·é‡è¯•")
    
    def _evaluate_accessibility_feature(self, feature: str) -> float:
        """è¯„ä¼°å¯è®¿é—®æ€§ç‰¹æ€§ï¼ˆæ¨¡æ‹Ÿè¯„åˆ†ï¼‰"""
        # è¿™é‡Œåº”è¯¥æœ‰å®é™…çš„å¯è®¿é—®æ€§æ£€æŸ¥é€»è¾‘
        # æš‚æ—¶è¿”å›æ¨¡æ‹Ÿè¯„åˆ†
        accessibility_scores = {
            "é¢œè‰²å¯¹æ¯”åº¦": 0.8,
            "å­—ä½“å¤§å°": 0.9,
            "é”®ç›˜å¯¼èˆª": 0.8,
            "ç§»åŠ¨ç«¯é€‚é…": 0.7
        }
        return accessibility_scores.get(feature, 0.7)
    
    def _evaluate_feedback_quality(self, feedback_type: str) -> float:
        """è¯„ä¼°åé¦ˆè´¨é‡ï¼ˆæ¨¡æ‹Ÿè¯„åˆ†ï¼‰"""
        # è¿™é‡Œåº”è¯¥æœ‰å®é™…çš„åé¦ˆè´¨é‡æ£€æŸ¥é€»è¾‘
        # æš‚æ—¶è¿”å›æ¨¡æ‹Ÿè¯„åˆ†
        feedback_scores = {
            "æˆåŠŸåé¦ˆ": 0.9,
            "é”™è¯¯åé¦ˆ": 0.8,
            "è¿›åº¦åé¦ˆ": 0.7,
            "å¸®åŠ©åé¦ˆ": 0.8
        }
        return feedback_scores.get(feedback_type, 0.7)

def run_user_experience_tests():
    """è¿è¡Œç”¨æˆ·ä½“éªŒæµ‹è¯•"""
    print("ğŸ¨ å¼€å§‹ç”¨æˆ·ä½“éªŒæµ‹è¯• - äº§å“æ˜“ç”¨æ€§ä¸ç”¨æˆ·æ»¡æ„åº¦éªŒè¯")
    print("=" * 60)
    
    # åˆ›å»ºæµ‹è¯•å¥—ä»¶
    suite = unittest.TestSuite()
    
    # æ·»åŠ æ‰€æœ‰ç”¨æˆ·ä½“éªŒæµ‹è¯•
    test_methods = [
        'test_input_validation_ux',
        'test_response_time_performance',
        'test_error_handling_ux',
        'test_loading_states_ux',
        'test_progressive_disclosure_ux',
        'test_accessibility_features',
        'test_user_journey_flow',
        'test_feedback_mechanisms'
    ]
    
    for method in test_methods:
        suite.addTest(TestUserExperience(method))
    
    # è¿è¡Œæµ‹è¯•
    runner = unittest.TextTestRunner(verbosity=2)
    result = runner.run(suite)
    
    # è¾“å‡ºæ€»ç»“
    print("\n" + "=" * 60)
    if result.wasSuccessful():
        print("ğŸ‰ ç”¨æˆ·ä½“éªŒæµ‹è¯•å…¨éƒ¨é€šè¿‡ï¼")
        print("âœ… è¾“å…¥éªŒè¯ç”¨æˆ·ä½“éªŒè‰¯å¥½")
        print("âœ… å“åº”æ—¶é—´æ€§èƒ½è¾¾æ ‡")
        print("âœ… é”™è¯¯å¤„ç†å‹å¥½")
        print("âœ… åŠ è½½çŠ¶æ€åˆç†")
        print("âœ… ä¿¡æ¯å±‚æ¬¡æ¸…æ™°")
        print("âœ… å¯è®¿é—®æ€§è€ƒè™‘å‘¨å…¨")
        print("âœ… ç”¨æˆ·æµç¨‹é¡ºç•…")
        print("âœ… åé¦ˆæœºåˆ¶å®Œå–„")
        print("ğŸ¯ äº§å“ç”¨æˆ·ä½“éªŒå·²è¾¾åˆ°å•†ä¸šåŒ–æ ‡å‡†ï¼")
    else:
        print("âŒ ç”¨æˆ·ä½“éªŒæµ‹è¯•å­˜åœ¨é—®é¢˜ï¼")
        print(f"å¤±è´¥æµ‹è¯•æ•°: {len(result.failures)}")
        print(f"é”™è¯¯æµ‹è¯•æ•°: {len(result.errors)}")
        print("ğŸš¨ éœ€è¦ä¼˜åŒ–ç”¨æˆ·ç•Œé¢å’Œäº¤äº’ä½“éªŒï¼")
    
    return result.wasSuccessful()

if __name__ == "__main__":
    success = run_user_experience_tests()
    sys.exit(0 if success else 1)
